# ğŸ¯ Phase 1 - Version MVP (Minimum Viable Product)

## ğŸ“‹ Vue d'ensemble

Cette version reprÃ©sente le **point de dÃ©part** du projet transVoicer avec les fonctionnalitÃ©s essentielles pour dÃ©montrer le concept de traduction vocale.

---

## âœ¨ FonctionnalitÃ©s Incluses

### Frontend
- âœ… Enregistrement audio via microphone
- âœ… Affichage transcription (texte original)
- âœ… Affichage traduction (texte traduit)
- âœ… Interface simple et fonctionnelle

### Backend Node.js
- âœ… Serveur Express basique
- âœ… WebSocket pour communication temps rÃ©el
- âœ… Route API traduction (`/api/translate`)
- âœ… IntÃ©gration API Gemini

### Backend Python
- âœ… API FastAPI basique
- âœ… Service STT avec Whisper (modÃ¨le "base")
- âœ… Transcription audio simple

---

## ğŸš€ Installation Phase 1

### PrÃ©requis
- Node.js 18+
- Python 3.8+
- ClÃ© API Google Gemini

### Ã‰tapes

#### 1. Frontend (React)

**DÃ©pendances minimales** :
```json
{
  "dependencies": {
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "vite": "^5.0.8",
    "@vitejs/plugin-react": "^4.2.1"
  }
}
```

**Installation** :
```bash
cd frontend
npm install
```

#### 2. Backend Node.js

**DÃ©pendances minimales** :
```json
{
  "dependencies": {
    "express": "^4.18.2",
    "ws": "^8.14.2",
    "axios": "^1.6.2",
    "@google/generative-ai": "^0.2.1",
    "dotenv": "^16.3.1"
  }
}
```

**Installation** :
```bash
npm install
```

#### 3. Backend Python

**DÃ©pendances minimales** :
```txt
fastapi>=0.104.1
uvicorn[standard]>=0.24.0
python-multipart>=0.0.6
openai-whisper>=20231117
torch>=2.1.0
librosa>=0.10.1
soundfile>=0.12.1
numpy>=1.26.0
```

**Installation** :
```bash
cd python
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

## ğŸ“ Structure Phase 1

### Frontend
```
frontend/src/
â”œâ”€â”€ main.jsx
â”œâ”€â”€ App.jsx                    # Version simple
â”œâ”€â”€ App.css                    # Styles basiques
â”œâ”€â”€ index.css                  # Reset simple
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ MicrophoneRecorder.jsx # Version basique
â”‚   â””â”€â”€ TranslationDisplay.jsx  # Version basique
â””â”€â”€ hooks/
    â”œâ”€â”€ useSpeechRecognition.js # Version simplifiÃ©e
    â””â”€â”€ useTranslation.js       # Version simplifiÃ©e
```

### Backend Node.js
```
server/
â”œâ”€â”€ index.js                   # Version simple
â””â”€â”€ translationService.js      # Version basique
```

### Backend Python
```
python/
â”œâ”€â”€ api.py                     # Version simple
â””â”€â”€ services/
    â””â”€â”€ speech_to_text.py     # Version simplifiÃ©e
```

---

## ğŸ”§ Configuration Phase 1

### Variables d'environnement

**`.env` (racine)** :
```env
PORT=3001
GEMINI_API_KEY=your_key_here
```

**`python/.env`** :
```env
PYTHON_API_PORT=8000
WHISPER_MODEL_SIZE=base
STT_LANGUAGE=pt
```

---

## ğŸ¨ Code SimplifiÃ© Phase 1

### Frontend - useSpeechRecognition (SimplifiÃ©)

```javascript
import { useState, useRef, useEffect } from 'react'

export const useSpeechRecognition = () => {
  const [isRecording, setIsRecording] = useState(false)
  const [error, setError] = useState(null)
  const wsRef = useRef(null)
  const mediaRecorderRef = useRef(null)
  const audioStreamRef = useRef(null)

  // Connexion WebSocket simple
  useEffect(() => {
    const ws = new WebSocket('ws://localhost:3001/ws')
    wsRef.current = ws
    
    ws.onopen = () => {
      console.log('WebSocket connectÃ©')
    }
    
    return () => {
      ws.close()
    }
  }, [])

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      audioStreamRef.current = stream
      
      const mediaRecorder = new MediaRecorder(stream)
      mediaRecorderRef.current = mediaRecorder
      
      mediaRecorder.ondataavailable = (event) => {
        if (wsRef.current?.readyState === WebSocket.OPEN) {
          wsRef.current.send(event.data)
        }
      }
      
      mediaRecorder.start(100)
      setIsRecording(true)
    } catch (err) {
      setError(err.message)
    }
  }

  const stopRecording = () => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop()
      setIsRecording(false)
      
      // Envoyer signal de fin
      if (wsRef.current?.readyState === WebSocket.OPEN) {
        wsRef.current.send(JSON.stringify({ type: 'end' }))
      }
      
      // ArrÃªter stream
      if (audioStreamRef.current) {
        audioStreamRef.current.getTracks().forEach(track => track.stop())
      }
    }
  }

  return {
    isRecording,
    startRecording,
    stopRecording,
    error,
    wsRef
  }
}
```

### Backend Node.js - index.js (SimplifiÃ©)

```javascript
const express = require('express')
const http = require('http')
const WebSocket = require('ws')
const { translateText } = require('./translationService')
const axios = require('axios')

const app = express()
const server = http.createServer(app)
const wss = new WebSocket.Server({ server })

app.use(express.json())

let audioChunks = []

wss.on('connection', (ws) => {
  console.log('Nouvelle connexion WebSocket')
  
  ws.on('message', async (data) => {
    try {
      // VÃ©rifier si c'est un signal de fin
      if (data.toString().startsWith('{')) {
        const message = JSON.parse(data.toString())
        if (message.type === 'end') {
          // Traiter l'audio accumulÃ©
          const audioBuffer = Buffer.concat(audioChunks)
          audioChunks = []
          
          // Envoyer Ã  l'API Python
          const formData = new FormData()
          formData.append('file', audioBuffer, { filename: 'audio.webm' })
          
          const response = await axios.post(
            'http://localhost:8000/api/stt/transcribe',
            formData,
            { headers: formData.getHeaders() }
          )
          
          const transcription = response.data.text
          
          // Envoyer transcription au client
          ws.send(JSON.stringify({
            type: 'transcription',
            text: transcription
          }))
          
          // Traduire
          const translatedText = await translateText(transcription, 'fr')
          
          // Envoyer traduction au client
          ws.send(JSON.stringify({
            type: 'translation',
            text: translatedText
          }))
        }
      } else {
        // Accumuler chunks audio
        audioChunks.push(Buffer.from(data))
      }
    } catch (error) {
      console.error('Erreur:', error)
      ws.send(JSON.stringify({
        type: 'error',
        message: error.message
      }))
    }
  })
})

app.post('/api/translate', async (req, res) => {
  try {
    const { text, targetLanguage } = req.body
    const translatedText = await translateText(text, targetLanguage || 'fr')
    res.json({ translatedText })
  } catch (error) {
    res.status(500).json({ error: error.message })
  }
})

server.listen(3001, () => {
  console.log('Serveur dÃ©marrÃ© sur le port 3001')
})
```

### Backend Python - speech_to_text.py (SimplifiÃ©)

```python
import whisper
import torch
import tempfile
import os

class SpeechToTextService:
    def __init__(self, model_size="base", language="pt"):
        self.model_size = model_size
        self.language = language
        
        # DÃ©tecter device
        device = "cuda" if torch.cuda.is_available() else "cpu"
        self.device = device
        
        # Charger modÃ¨le
        self.model = whisper.load_model(model_size, device=device)
    
    def transcribe(self, audio_path: str) -> dict:
        # Transcription simple
        result = self.model.transcribe(
            audio_path,
            language=self.language,
            task="transcribe"
        )
        
        return {
            "text": result["text"].strip(),
            "language": result.get("language", self.language)
        }
```

---

## ğŸš€ DÃ©marrage Phase 1

### 1. DÃ©marrer le service Python
```bash
cd python
source venv/bin/activate
python api.py
```

### 2. DÃ©marrer le serveur Node.js
```bash
npm start
# ou
node server/index.js
```

### 3. DÃ©marrer le frontend
```bash
cd frontend
npm run dev
```

### 4. Ouvrir dans le navigateur
```
http://localhost:3000
```

---

## âš ï¸ Limitations Phase 1

### FonctionnalitÃ©s non incluses
- âŒ Upload de fichiers audio
- âŒ Text-to-Speech (lecture vocale)
- âŒ MÃ©triques et logs techniques
- âŒ PrÃ©-traitement audio avancÃ©
- âŒ Gestion d'erreurs robuste
- âŒ Reconnexion automatique WebSocket
- âŒ ThÃ¨me sombre professionnel
- âŒ Responsive design avancÃ©

### Simplifications
- Pas de gestion sessions avancÃ©e
- Pas de retry automatique
- Pas de fallback simulation
- Pas de validation audio complexe
- Pas de rechargement modÃ¨le
- Pas de threading.Lock

---

## ğŸ“ Prochaines Ã‰tapes

Pour migrer vers **Phase 2** (version finale), consultez :
- `ARCHITECTURE_PHASES.md` : Guide de migration complet
- `QUICKSTART.md` : Guide de la version finale

---

## âœ… Checklist Phase 1

### FonctionnalitÃ©s de base
- [x] Enregistrement audio
- [x] Transcription (STT)
- [x] Traduction (Gemini)
- [x] Affichage rÃ©sultats
- [x] Communication WebSocket
- [x] API REST traduction

### Tests
- [ ] Test enregistrement audio
- [ ] Test transcription
- [ ] Test traduction
- [ ] Test WebSocket
- [ ] Test erreurs basiques

---

## ğŸ“ Objectifs PÃ©dagogiques Phase 1

Cette version permet de comprendre :
1. **Concepts de base** : WebSocket, MediaRecorder, API REST
2. **Architecture simple** : Frontend â†’ Backend â†’ Services
3. **IntÃ©gration services** : Whisper, Gemini
4. **Flux de donnÃ©es** : Audio â†’ Transcription â†’ Traduction

Une fois ces concepts maÃ®trisÃ©s, vous pouvez progresser vers **Phase 2** pour dÃ©couvrir les optimisations et fonctionnalitÃ©s avancÃ©es.


